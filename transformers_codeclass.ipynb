{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NLP Transfer Learning with ðŸ¤— Transformers\n",
    "\n",
    "#### Data Labs Code Class Tues., 10/5\n",
    "\n",
    "#### Sam Bestvater | Computational Social Scientist\n",
    "\n",
    "In this notebook, I introduce and demonstrate the Hugging Face (ðŸ¤—) `Transformers` package, a Python package for transfer learning in neural NLP that is quickly becoming one of the essential tools in the field. \n",
    "\n",
    "This notebook draws heavily from the documentation for the package. For more info, see:\n",
    "- [huggingface.co/transformers](https://huggingface.co/transformers)\n",
    "- [huggingface.co/course](https://huggingface.co/course)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setup\n",
    "(what packages & dependencies to install if you want to follow along with the notebook)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install transformers --user\n",
    "#!pip install datasets --user\n",
    "#!pip install sentencepiece --user\n",
    "#!pip install torch==1.9.1+cpu torchvision==0.10.1+cpu torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html --user\n",
    "#!pip install torchinfo --user"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. What is a Transformer model, and how does it work?\n",
    "\n",
    "Let's establish some other definitions first:\n",
    "- neural networks: what if instead of using just one logistic regression, we chained a bunch of them together in multiple layers? Turns out that gives us a really flexible model for working with super high dimensional data types like text or images. [(3Blue1Brown has a great series on YouTube if you want to actually understand how neural nets work in any detail.)](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- model architecture: the configuration of units and connections that define a specific neural network (i.e. the overall shape of the model).\n",
    "- model checkpoint: the parameters (weights and biases) that the model learns through training. (i.e. the trained model)\n",
    "- transfer learning: taking a model checkpoint trained on one task or dataset and using it for another task or dataset (either as-is, or modified to adapt it to that task).\n",
    "- language modeling: statistically modeling how words in natural language relate to each other, often for next-word or missing-word prediction (like predictive keyboards on smartphones). It turns out that language models are good to use for transfer learning, because the things they learn about the underlying interdependencies of language are widely useful for a variety of NLP tasks. \n",
    "\n",
    "A *Transformer* is a kind of neural network architecture designed for language modeling, first introduced by researchers at Google in 2017 [(Vaswani et al. 2017)](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). The big innovation of the transformer architecture is that it is much more computationally efficient than the other types of neural network architectures used for language modeling up to that point (mostly RNNs and LSTMs). The efficiency of the architecture meant that researchers could train much larger models on much larger quantities of data. Another research team at Google followed up the original Transformer paper by releasing a model called BERT, and open-sourcing the model checkpoints for other researchers to use [(Devlin et al. 2018)](https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ). The BERT model has 336M trainable parameters and recognizes a vocabulary of over 30,000 English terms. It was trained on two massive text corpora, the WikiText-103 corpus (which is basically all of wikipedia) and the BookCorpus. \n",
    "\n",
    "The publication of BERT launched a new era in NLP research, as these huge language models facilitated much more effective transfer learning than anything that had come before. Most tasks in NLP can be improved through the use of BERT or another transformer model like it.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. What kinds of things can you do with Transformers?\n",
    "\n",
    "A bunch of things!\n",
    "\n",
    "- Sequence classification: (sentiment analysis, spam detection, grammar checking, logical entailment, etc.)\n",
    "- Token classification: (POS-tagging, named entity recognition, masked token prediction, etc.)\n",
    "- Text generation: (GPT stuff & things)\n",
    "- Sequence transformation: (document summarization, document translation, etc.)\n",
    "- Text representation: (producing contextual embeddings, representing multilingual texts, etc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. What is ðŸ¤—?\n",
    "\n",
    "ðŸ¤— (Hugging Face) exists in the form that it does today as a result of competition between the two most common deep learning frameworks for Python, `PyTorch` and `TensorFlow`. Researchers at Google first introduced the Transformer architecture [(Vaswani et al. 2017)](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) and also developed BERT, one of the early general-purpose Transformer language models [(Devlin et al. 2018)](https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ). Google also owns TensorFlow, so when the BERT paper was published in 2018, the research team open sourced the pre-trained models as TensorFlow objects (which are all still available on their [GitHub](https://github.com/google-research/bert), BTW). The developers at Hugging Face wanted to use BERT models in their research, but primarily worked in PyTorch, so they converted the models and released a python package called `pytorch-pretrained-bert` that was simply a set of commands for downloading the converted BERT models as PyTorch objects. This quickly became hugely popular, and the de-facto default implementation for other researchers who wanted to use BERT models with PyTorch (which was a lot of people). As more pre-trained transformers were published, Hugging Face started hosting those as well, and began developing a general set of tools to work with these models. Around this time they also changed the name of their python package to the more general `transformers`.\n",
    "\n",
    "Today, Hugging Face maintains several python packages.\n",
    "\n",
    "- `transformers` provides functions for downloading and implementing Transformer models for NLP tasks. All the tools now work in both PyTorch and TensorFlow, and many models are now available for both frameworks as well. \n",
    "- `huggingface_hub` integrates with the online repository of models that Hugging Face hosts ([huggingface.co/models](https://huggingface.co/models)) and allows users to upload their own fine-tuned transformers. There are currently 17,196 model checkpoints hosted on the Hugging Face Hub.\n",
    "- `datasets` provides easy access to an online library of major public datasets for NLP, and also provides an efficient data format for use with `transformers` functions and models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Cool! Let's see some examples!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 At a high level: using the Pipeline API\n",
    "\n",
    "The `Pipeline` API abstracts away most of the technical detail of the underlying models, allowing us to focus on using pre-trained models for specific, well-defined tasks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "print('CUDA enabled') if torch.cuda.is_available() else print('CPU only')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA enabled\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentiment Analysis\n",
    "(asking an encoder model to determine if the overall tone of a text is positive or negative)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\"Everyone on Data Labs is amazing and brilliant!\", \n",
    "    \"This code class is so boring, I wish Sam would stop talking.\"],\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998788833618164},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997872114181519}]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zero-shot classification\n",
    "(asking an encoder model to classify a text from a list of labels it's never seen before)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    [\"The 2020 election featured dramatic increases in lawmaker posts and audience engagement\",\n",
    "    \"Majority in U.S. Says Public Health Benefits of COVID-19 Restrictions Worth the Costs\"],\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\", \"health\"],\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': 'The 2020 election featured dramatic increases in lawmaker posts and audience engagement',\n",
       "  'labels': ['politics', 'business', 'health', 'education'],\n",
       "  'scores': [0.9503316879272461,\n",
       "   0.026905445381999016,\n",
       "   0.012079885229468346,\n",
       "   0.010683040134608746]},\n",
       " {'sequence': 'Majority in U.S. Says Public Health Benefits of COVID-19 Restrictions Worth the Costs',\n",
       "  'labels': ['health', 'business', 'politics', 'education'],\n",
       "  'scores': [0.977282702922821,\n",
       "   0.01176140084862709,\n",
       "   0.008106384426355362,\n",
       "   0.0028495402075350285]}]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text generation\n",
    "(asking a decoder model to produce more text from a given input)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In a hole in the ground, there lived a Hobbit.\",\n",
    "         num_return_sequences = 2, # how many examples you'd like the model to produce\n",
    "         max_length = 50) # how long these should be"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'In a hole in the ground, there lived a Hobbit. Or two. No doubt about it â€“ and a very good story and good ending so far.\\n\\n\\nI had so much fun reading, my wife brought me her first book of The Hobbit'},\n",
       " {'generated_text': \"In a hole in the ground, there lived a Hobbit. The wizard that created the world, he had created a dragon, a creature whose eyes were as red as lightning, whose mind and soul, and whose eyes and limbs were like a dragon's\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summarization\n",
    "(asking an encoder-decoder model to produce a shorter version of a given input)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "            Although gymnast Simone Bilesâ€™ medal count fell slightly short of the \n",
    "            sports worldâ€™s lofty expectations in the Tokyo 2020 Olympic Games, \n",
    "            she dominated among U.S. Olympians in the number of times her handle, \n",
    "            @Simone_Biles, was mentioned on Twitter.\n",
    "\n",
    "            Pew Research Center captured the Twitter handles of every athlete who \n",
    "            listed a profile on the official Team USA page and looked at tweets \n",
    "            from the broader Twitter audience that directly mentioned those handles \n",
    "            during the Games. Here are some key takeaways for how the public engaged \n",
    "            with Team USA on Twitter.\n",
    "            \n",
    "            All told, 598 athletes were listed on the Team USA website at the start \n",
    "            of the Games. And 438 of them (73% of the total) included a Twitter handle \n",
    "            in their athlete profile. From July 21 through Aug. 9, 2021 â€“ the Games \n",
    "            themselves, postponed from the year before, were held July 23 to Aug. 8 â€“ \n",
    "            more than 900,000 different Twitter accounts directly mentioned the handles \n",
    "            of U.S. Olympians in more than 2.1 million tweets. The vast majority (90%) \n",
    "            of those athlete accounts were mentioned at least once during that time.\n",
    "\n",
    "            These mentions were especially concentrated on a few key dates. Nearly a \n",
    "            third (31%) of all athlete mentions occurred during the three days of July 27-29, \n",
    "            a period that included the womenâ€™s team and individual gymnastics finals and \n",
    "            swimmer Katie Ledecky winning the gold medal in the 1,500-meter freestyle.\n",
    "            \"\"\",\n",
    "           max_length = 100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/bestvater/anaconda3/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': ' Pew Research Center captured the Twitter handles of every athlete who listed a profile on the official Team USA page and looked at tweets from the broader Twitter audience . From July 21 through Aug. 9, 2021 â€“ the Games were held July 23 to Aug. 8 â€“ more than 900,000 different Twitter accounts directly mentioned the handles of U.S. Olympians in more than 2.1 million tweets .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Translation\n",
    "(asking an encoder-decoder model to translate an input from a specified source language to a specified target language)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "translator(\"My name is Sam and I work on the Data Labs team at Pew Research Center in Washington DC.\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'translation_text': \"Je m'appelle Sam et je travaille au sein de l'Ã©quipe Data Labs du Pew Research Center Ã  Washington DC.\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Named entity recognition (NER)\n",
    "(asking an encoder model to extract entities such as persons, locations, or organizations from an input sequence)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "ner = pipeline(\"ner\",\n",
    "              grouped_entities = True # Allows n-gram entities\n",
    "              )\n",
    "ner(\"My name is Sam and I work on the Data Labs team at Pew Research Center in Washington DC.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/bestvater/anaconda3/lib/python3.7/site-packages/transformers/pipelines/token_classification.py:155: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  f'`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99874526,\n",
       "  'word': 'Sam',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9987684,\n",
       "  'word': 'Data Labs',\n",
       "  'start': 33,\n",
       "  'end': 42},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99569225,\n",
       "  'word': 'Pew Research Center',\n",
       "  'start': 51,\n",
       "  'end': 70},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99902904,\n",
       "  'word': 'Washington DC',\n",
       "  'start': 74,\n",
       "  'end': 87}]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question Answering\n",
    "(providing an encoder model with a context statement and asking it questions based on that context)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=[\"What is my name?\", \"Where do I work?\"],\n",
    "    context=\"My name is Sam and I work on the Data Labs team at Pew Research Center in Washington DC.\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 0.9965927004814148, 'start': 11, 'end': 14, 'answer': 'Sam'},\n",
       " {'score': 0.30844539403915405,\n",
       "  'start': 51,\n",
       "  'end': 70,\n",
       "  'answer': 'Pew Research Center'}]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction\n",
    "(using an encoder model to encode an input text into a contextual embedding that can be used for other NLP tasks)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "encoder = pipeline(\"feature-extraction\")\n",
    "embedding = encoder(\"Hobbits have hairy feet.\")\n",
    "\n",
    "embedding = np.array(embedding)\n",
    "\n",
    "embedding.shape"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 9, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "encoder = pipeline(\"feature-extraction\")\n",
    "embedding = encoder([\"Hobbits have hairy feet.\",\n",
    "                     \"Does BERT know what a Hobbit is?\"])\n",
    "\n",
    "embedding = np.array(embedding)\n",
    "\n",
    "embedding.shape"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2, 14, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# remove previous models to free up memory:\n",
    "import gc\n",
    "\n",
    "del classifier\n",
    "del generator\n",
    "del summarizer\n",
    "del translator\n",
    "del ner\n",
    "del question_answerer\n",
    "del encoder\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "\n",
    "    print(t)\n",
    "    print(f)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12636061696\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Less abstraction: using the Trainer API\n",
    "\n",
    "The `Pipeline` API makes certain pre-defined tasks really really easy, but it's doing a lot under the hood. \n",
    "\n",
    "Moving from raw text to predictions requires not just a model, but also a tokenizer that produces inputs the model understands, and a post-processing step that can convert the raw model outputs into something interpretable. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Under the hood](https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sometimes we need models for tasks outside of these pre-determined pipelines. For that, we can use the `Trainer` API, which gives us more control over these underlying components of the model and allows us to fine-tune pre-trained models on new, task-specific data.\n",
    "\n",
    "Let's say we want to fine-tune a transformer for a custom classification task, such as identifying the stance of tweets about the Kavanaugh confirmation hearings (see [Bestvater & Monroe, working paper](https://bestvater.github.io/pdfs/BestvaterMonroe_SentimentIsNotStance.pdf) -- absolutely shameless self-promotion.) \n",
    "\n",
    "Let's load some data. Probably the most common way to do this in Python is with Pandas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "kav_tweets = pd.read_csv('https://github.com/bestvater/misc/raw/master/kavanaugh_tweets_stance.csv',\n",
    "                         usecols = ['text', 'stance'])\n",
    "\n",
    "kav_tweets['stance'] = np.where(kav_tweets['stance'] == 1, # make the labels more informative\n",
    "                                'Supports the Kavanaugh confirmation', \n",
    "                                'Opposes the Kavanaugh confirmation')\n",
    "\n",
    "kav_tweets = kav_tweets.sample(n = 100, random_state = 101) # subset so training doesn't take forever\n",
    "\n",
    "kav_tweets.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>'He's a Liar': Watters Rips RI Sen. Whitehouse...</td>\n",
       "      <td>Supports the Kavanaugh confirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>HERE WE GO - WHAT IS IT WITH SENATORS FROM NEW...</td>\n",
       "      <td>Supports the Kavanaugh confirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>JUST IN: Democrats float idea of impeaching Ka...</td>\n",
       "      <td>Supports the Kavanaugh confirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>She already has her mind made up...She knows A...</td>\n",
       "      <td>Opposes the Kavanaugh confirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Only 31% of Americans believe that Brett Kavan...</td>\n",
       "      <td>Opposes the Kavanaugh confirmation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "795   'He's a Liar': Watters Rips RI Sen. Whitehouse...   \n",
       "665   HERE WE GO - WHAT IS IT WITH SENATORS FROM NEW...   \n",
       "1389  JUST IN: Democrats float idea of impeaching Ka...   \n",
       "416   She already has her mind made up...She knows A...   \n",
       "147   Only 31% of Americans believe that Brett Kavan...   \n",
       "\n",
       "                                   stance  \n",
       "795   Supports the Kavanaugh confirmation  \n",
       "665   Supports the Kavanaugh confirmation  \n",
       "1389  Supports the Kavanaugh confirmation  \n",
       "416    Opposes the Kavanaugh confirmation  \n",
       "147    Opposes the Kavanaugh confirmation  "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's convert this to a hugging face dataset though--this will give us some specific functionality that we don't get with Pandas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "kav_tweets = Dataset.from_pandas(kav_tweets)\n",
    "\n",
    "kav_tweets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'stance', '__index_level_0__'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, now we can get started. First we need to decide on a model checkpoint. This is a sequence classification task, so we want one of the encoder models. I'm going to go with `DistilBERT` because it's relatively lightweight."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "checkpoint = 'distilbert-base-uncased'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizing/Preprocessing\n",
    "\n",
    "Once we've chosen a model to use, we need to do some preprocessing to convert the raw texts we want to classify into inputs that our `DistilBERT` model will recognize. We do this using a `tokenizer` function, which is provided along with every pre-trained model in the hugging face library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, # loads the tokenizer for the model we specify\n",
    "                                         model_max_len = 128) # set the dimension of the input vector. Default is 512\n",
    "\n",
    "tokenizer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's what happens when we pass a raw text to this tokenizer:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer('Hello. This is Sono from work.')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1012, 2023, 2003, 2365, 2080, 2013, 2147, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I'm going to wrap that in another function so I can easily apply it to the whole dataset at once using `map()`. This isn't strictly necessary, but for big datasets it can speed things up a lot. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def tokenize_function(input_dataset):\n",
    "    return tokenizer(input_dataset['text'],\n",
    "                     padding = 'max_length', # will pad documents shorter than 128 tokens\n",
    "                     truncation = True ) # will truncate documents longer than 128 tokens\n",
    "\n",
    "kav_tweets = kav_tweets.map(tokenize_function, batched = True)\n",
    "\n",
    "kav_tweets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 66.96ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'attention_mask', 'input_ids', 'stance', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now our dataset has the `input_ids` and `attention_mask` vectors that we'll need to pass to the model. Before we do that, though, let's process the `stance` column, which contains our labels. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "kav_tweets = kav_tweets.rename_column('stance', 'labels')\n",
    "kav_tweets = kav_tweets.class_encode_column('labels') # tells the model that this is the labels column\n",
    "\n",
    "kav_tweets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 865.88ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.52ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'attention_mask', 'input_ids', 'labels', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Model\n",
    "\n",
    "Now that we've got correctly pre-processed texts, we can load and fine-tune our `DistilBERT` model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, # tell from_pretrained to load distilbert\n",
    "                                                           num_labels = 2) # tell it to add a binary classifier head"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, that warning message is telling us that we need to TRAIN this model before it will be useful to us. We can do that using a `Trainer()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `TrainingArguments()` class lets us specify all of the parameters that get passed to a `Trainer()`. Most of the defaults are sensible, but we're going to tweak a couple of things:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "training_args = TrainingArguments(output_dir = './distilbert_model', # specify the directory where our fine-tuned model will be saved\n",
    "                                  overwrite_output_dir = True, # so it doesn't make a new copy every time I run this\n",
    "                                  evaluation_strategy = 'no', # check model performance every epoch\n",
    "                                  logging_strategy = 'no', # we're not going to log the model's performance anywhere though\n",
    "                                  per_device_train_batch_size = 16, # how many inputs to process at once during training\n",
    "                                  num_train_epochs = 2 # how many times to pass through the entire training set\n",
    "                                 )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = kav_tweets,\n",
    "    eval_dataset = kav_tweets # don't evaluate on the training data, bad.\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "trainer.train() # this takes a little while on CPU"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: Currently logged in as: sbestvater (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./distilbert_model</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sbestvater/huggingface\" target=\"_blank\">https://wandb.ai/sbestvater/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sbestvater/huggingface/runs/27fq9cxs\" target=\"_blank\">https://wandb.ai/sbestvater/huggingface/runs/27fq9cxs</a><br/>\n",
       "                Run data is saved locally in <code>/home/bestvater/Desktop/code-classes/wandb/run-20211004_131157-27fq9cxs</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:05<00:00,  2.96it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:05<00:00,  2.40it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 12.0542, 'train_samples_per_second': 16.592, 'train_steps_per_second': 1.161, 'train_loss': 0.6851015772138324, 'epoch': 2.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14, training_loss=0.6851015772138324, metrics={'train_runtime': 12.0542, 'train_samples_per_second': 16.592, 'train_steps_per_second': 1.161, 'train_loss': 0.6851015772138324, 'epoch': 2.0})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, so that's a fine-tuned DistilBERT model. Let's try getting classifications for a couple new texts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "new_input = tokenizer([\"I don't think Brett Kavanaugh should be on the Supreme Court.\",\n",
    "                       \"Quit stalling and confirm Kavanaugh already!\"],\n",
    "                     padding = 'max_length',\n",
    "                     truncation = True,\n",
    "                     return_tensors = 'pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "outputs = model(**new_input)\n",
    "\n",
    "outputs"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da403650dd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         )\n\u001b[1;32m    634\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (bs, seq_len, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         return self.transformer(\n\u001b[1;32m    490\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model returns logits, which aren't particularly interpretable on their own. Let's transform those to probabilities:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) #convert logits to probabilities\n",
    "\n",
    "predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, those aren't very confident predictions, but we also only fine-tuned the model on 100 documents. Luckily, through some cooking show-style magic, I've saved a local copy of the model trained on the full dataset. Let's look at that instead.\n",
    "\n",
    "We can load local models by passing `AutoModelForSequenceClassification.from_pretrained()` the file path, like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bestvater/distilbert-kav-stance', # tell from_pretrained to load from model hub\n",
    "                                                           num_labels = 2) # tell it to add a binary classifier head"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file bestvater/distilbert-kav-stance/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bestvater/distilbert-kav-stance/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at bestvater/distilbert-kav-stance.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "outputs = model(**new_input)\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) #convert logits to probabilities\n",
    "\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9226, 0.0774],\n",
       "        [0.0476, 0.9524]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice. Now we have a custom classifier that can identify the stance of tweets about the Kavanaugh hearings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Even less abstraction: Transformers as PyTorch or TensorFlow model objects\n",
    "If we really want the ability to tweak anything at all, models on the Transformers hub are all just PyTorch objects (many are also availabile in TensorFlow). If we want to, we can just load the pretrained models and work with them directly in those frameworks, ignoring the huggingface APIs altogether. \n",
    "\n",
    "As a really quick proof of this, let's load the `summary` function from `torchinfo`. This is a simple function that prints out an architechture summary of any PyTorch model. We can apply this to our trained DistilBERT model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I won't go into any more PyTorch detail today, but it's worth knowing that you can work with these models this way if you want to."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. A word of warning on algorithmic bias\n",
    "\n",
    "Transformers are cool and useful because they \"learn\" components of the complex interdependencies of natural language. But it's important to remember that they do this by being trained on massive amounts of text produced by humans, who have implicit biases. Large language models often learn the biases present in their training data. \n",
    "\n",
    "Here is an example, using the `fill-mask` pipeline, which is essentially asking the model to play madlibs and fill in a missing word in an input string."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fill_blank = pipeline(\"fill-mask\", model = 'bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = fill_blank(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = fill_blank(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow. That is *obnoxiously* sexist.\n",
    "\n",
    "For this example we used the `bert-base-uncased` model, which is trained on the English Wikipedia and BookCorpus datasets. The BookCorpus in particular includes a lot of older texts, so that might be the source for some of this bias, but some other large language models are trained on more contemporary texts scraped from all over the internet, which can produce other, potentially worse issues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "4263d3458c891f11e38706bfd67abf26ef3f9f612eff080d0f3ea4d43facc11e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}